# AuraCode: Models, Pricing, and the Credit System

This document explains how Large Language Models (LLMs) are used within the AuraCode framework, how the credit system works, and how you can configure and optimize model usage for your agents.

## ðŸ”‘ Core Integration: OpenRouter

AuraCode is designed to be model-agnostic through its integration with **[OpenRouter](https://openrouter.ai/)**. Instead of being locked into a single provider like OpenAI or Google, AuraCode can access hundreds of different LLMs through a unified API.

To use the system, you **must** provide an `OPENROUTER_API_KEY` in your `.env` file.

```env
OPENROUTER_API_KEY="sk-or-v1-..."
```

## ðŸ’° The Credit System

AuraCode operates on an internal credit system to manage the cost of LLM API calls. This abstraction allows for flexible pricing and provides users with clear visibility into their usage.

-   **1000 Free Credits:** New users are automatically granted 1,000 credits upon registration to get started.
-   **Cost is per-call:** Credits are deducted after each interaction with an agent that requires an LLM call.
-   **Dynamic Pricing:** The credit cost of an interaction is not fixed. It depends on three factors:
    1.  **The specific model used.**
    2.  **The number of input tokens** (the size of the prompt sent to the model).
    3.  **The number of output tokens** (the length of the response generated by the model).

### How Credit Costs are Calculated

The pricing logic is defined in `agent_manager.py` and `ceaf_adapter.py`. It follows a simple formula:

1.  **API Cost (USD):** We look up the model's cost per million input/output tokens from the `MODEL_API_COSTS_USD` dictionary. We calculate the raw API cost in USD.
2.  **Profit Markup:** A `PROFIT_MARKUP` (e.g., `3.0x`) is applied to the raw cost. This covers operational expenses and creates a business model.
3.  **Conversion to Credits:** The final USD cost is multiplied by `CREDITS_PER_DOLLAR` (e.g., `500`) to get the final credit cost. The result is rounded to the nearest whole number, with a minimum cost of 1 credit.

**Example:**
-   An interaction uses `gpt-4o-mini` (cost: $0.15/M input, $0.60/M output).
-   It processes 2,000 input tokens and generates 500 output tokens.
-   Raw cost: `($0.0003 + $0.0003) = $0.0006`
-   Markup cost: `$0.0006 * 3.0 = $0.0018`
-   Credit cost: `$0.0018 * 500 = 0.9` -> **1 credit**

### Model Tiers and Pricing

The `MODEL_COSTS` dictionary in `agent_manager.py` groups models into tiers to help users make informed decisions. The credit cost is a simplified representation of the underlying API cost.

| Tier | Example Models | Relative Cost | Use Case |
| :--- | :--- | :--- | :--- |
| **Free / Beta** | `deepseek-chat-v3-0324:free` | 1 credit (nominal) | Experimentation, testing, low-stakes tasks. |
| **Economy** | `mistral-nemo`, `gpt-4o-mini` | 2 credits | Excellent balance of cost and performance for most chat applications. |
| **Standard** | `gemini-2.5-flash` | 4-9 credits | Good performance for more complex reasoning. |
| **Premium** | `gpt-4o`, `gemini-2.5-pro` | 25 credits | High-end models for tasks requiring maximum intelligence and reasoning. |
| **Elite** | `claude-3.7-sonnet` | 40-50 credits | Top-tier, most expensive models for mission-critical applications. |

*Note: These credit values are examples and can be configured in `agent_manager.py`.*

---

## ðŸ”§ How to Change an Agent's Model

You can change the model an agent uses in two ways:

### 1. During Agent Creation

When creating a new agent via the API (`POST /agents/create`), you can specify the model in the request body.

```json
{
  "name": "My Gemini Agent",
  "persona": "An agent powered by Google's Gemini.",
  "detailed_persona": "...",
  "model": "openrouter/google/gemini-2.5-pro"
}
```
If no model is specified, it will default to `openrouter/openai/gpt-4o-mini`.

### 2. Updating an Existing Agent

You can change the model of an existing agent you own using the API.

-   **Endpoint:** `PUT /agents/{agent_id}/profile`
-   **Request Body:**
    ```json
    {
      "model": "openrouter/anthropic/claude-3.7-sonnet"
    }
    ```

### 3. Per-Session Overrides (For Developers)

For advanced use cases, you can override an agent's default model for a single chat session. This is done by passing the `session_overrides` object in the chat request.

-   **Endpoint:** `POST /agents/{agent_id}/chat`
-   **Request Body:**
    ```json
    {
      "message": "Answer this question using Grok.",
      "session_id": "some-session-123",
      "session_overrides": {
        "model": "openrouter/x-ai/grok-4"
      }
    }
    ```
The agent will use `grok-4` for this specific interaction and then revert to its default model for subsequent chats in the same session (unless overridden again). The credit cost will be calculated based on the overridden model (`grok-4`).

---

## ðŸ”¬ LLM Calls Per Interaction: NCF vs. CEAF

The number of LLM callsâ€”and therefore the costâ€”varies significantly between the two agent architectures.

### NCF (Narrative Context Framing) Agent

An NCF agent is highly efficient and designed to minimize costs.

1.  **Intent Routing (1 very small call):** A quick, cheap model (`gpt-4o-mini`) is used to classify the user's intent (e.g., `conversation`, `image_generation`). This determines if the query should be handled by the main agent or a specialist.
2.  **Main Response (1 primary call):** This is the main LLM call that generates the agent's response. It uses the agent's configured model (e.g., `gpt-4o`, `claude-sonnet`). The prompt for this call is very large and context-rich, containing the persona, RAG results, and chat history.
3.  **Aura Reflector (1 small background call):** After the response is sent, a background task uses a cheap model (`gpt-4o-mini`) to analyze the interaction and decide if any new memories should be created. **This call does not delay the user's response.**
4.  **Live Memory Analysis (1 small background call, optional):** If the agent is configured to contribute to the Live Memory, another cheap background call is made to extract an anonymized insight.

**Total for a standard NCF chat:** **~2 LLM calls** visible to the user (Intent + Main Response), plus 1-2 cheap, non-blocking calls in the background.

### CEAF (Coherent Emergence Agent Framework) Agent

A CEAF agent is a research-oriented system that performs significantly more reasoning per turn, making it more expensive. Its cost is directly tied to its "Precision Mode."

#### **Fast Mode (Default)**

-   **LLM Calls:** **1 primary call**.
-   **Process:** Similar to NCF, it constructs a large, context-rich prompt and makes a single call to the agent's configured model to generate a response.
-   **Cost:** Comparable to a standard NCF interaction.

#### **Precision Mode (DeepConf Algorithm)**

When "Precision Mode" is activated (e.g., by the user saying "be more precise"), the ORA node in the CEAF system initiates the **DeepConf algorithm**.

-   **LLM Calls:** **Variable, from 4 to 12+ parallel calls.**
-   **Process:**
    1.  **Warmup Phase (e.g., 3 calls):** The agent generates multiple "draft" responses in parallel to establish a confidence baseline.
    2.  **Final Phase (e.g., 5-9 calls):** The agent generates more parallel responses, stopping any "thought path" that deviates too far from the established consensus.
    3.  **Final Answer:** A final answer is selected from the valid, high-confidence thought paths.
-   **Cost:** **Significantly higher** than Fast Mode due to the multiple parallel LLM calls. This mode is designed for tasks requiring maximum deliberation, reasoning, and accuracy, where cost is a secondary concern.

### Summary of Calls per Interaction

| Agent Type | Mode | User-Facing LLM Calls | Background Calls (Cheap) | Relative Cost |
| :--- | :--- | :--- | :--- | :--- |
| **NCF** | Standard | 1 (Intent) + 1 (Main) | 1-2 | **Low** |
| **CEAF** | Fast Mode | 1 (Main) | 1-2 | **Low** |
| **CEAF** | Precision Mode | **4 to 12+** (DeepConf) | 1-2 | **Very High** |

By understanding this breakdown, developers can choose the right agent architecture and model for their use case, balancing performance, intelligence, and cost.
